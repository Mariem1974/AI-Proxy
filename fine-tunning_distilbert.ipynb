{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-02T21:55:18.892666Z",
     "iopub.status.busy": "2025-10-02T21:55:18.892383Z",
     "iopub.status.idle": "2025-10-02T21:55:18.898049Z",
     "shell.execute_reply": "2025-10-02T21:55:18.897146Z",
     "shell.execute_reply.started": "2025-10-02T21:55:18.892645Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "TRAIN_FILE = \"/kaggle/input/fine-tune3/train_prompts.csv\"\n",
    "VALIDATION_FILE = \"/kaggle/input/fine-tune3/validation_prompts.csv\"\n",
    "TEST_FILE = \"/kaggle/input/fine-tune3/test_prompts.csv\"\n",
    "MAX_LENGTH = 128\n",
    "SAVE_PATH = \"./prompt_injection_distilbert_v3\"\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T21:55:23.601424Z",
     "iopub.status.busy": "2025-10-02T21:55:23.600661Z",
     "iopub.status.idle": "2025-10-02T21:55:23.607980Z",
     "shell.execute_reply": "2025-10-02T21:55:23.607134Z",
     "shell.execute_reply.started": "2025-10-02T21:55:23.601383Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Checking Hardware Setup ---\n",
      "Running on default strategy (likely CPU or GPU)\n",
      "Number of replicas (cores/devices): 1\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ✅ Step 1: Check Hardware and Setup\n",
    "# ==============================================================================\n",
    "print(\"--- 1. Checking Hardware Setup ---\")\n",
    "try:\n",
    "    # Set up GPU or TPU strategy if available (common in Kaggle)\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "    print(\"Running on TPU:\", tpu.master())\n",
    "except ValueError:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"Running on default strategy (likely CPU or GPU)\")\n",
    "\n",
    "print(\"Number of replicas (cores/devices):\", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T21:55:28.234470Z",
     "iopub.status.busy": "2025-10-02T21:55:28.234100Z",
     "iopub.status.idle": "2025-10-02T21:55:30.066379Z",
     "shell.execute_reply": "2025-10-02T21:55:30.065393Z",
     "shell.execute_reply.started": "2025-10-02T21:55:28.234437Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 2. Loading Data ---\n",
      "Loaded Train: 209134 samples | Val: 26142 | Test: 26142\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ✅ Step 2: Load Data\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 2. Loading Data ---\")\n",
    "\n",
    "if not all(os.path.exists(f) for f in [TRAIN_FILE, VALIDATION_FILE, TEST_FILE]):\n",
    "    print(f\"Error: One or more data files ({TRAIN_FILE}, {VALIDATION_FILE}, {TEST_FILE}) not found.\")\n",
    "    print(\"Please ensure you have created and saved these files first.\")\n",
    "    exit()\n",
    "\n",
    "try:\n",
    "    df_train = pd.read_csv(TRAIN_FILE)\n",
    "    df_val = pd.read_csv(VALIDATION_FILE)\n",
    "    df_test = pd.read_csv(TEST_FILE)\n",
    "    \n",
    "    # Extract text and labels\n",
    "    train_texts = df_train['prompt']\n",
    "    train_labels = df_train['label']\n",
    "    \n",
    "    val_texts = df_val['prompt']\n",
    "    val_labels = df_val['label']\n",
    "    \n",
    "    test_texts = df_test['prompt']\n",
    "    test_labels = df_test['label']\n",
    "    \n",
    "    print(f\"Loaded Train: {len(train_texts)} samples | Val: {len(val_texts)} | Test: {len(test_texts)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    exit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T21:55:33.238284Z",
     "iopub.status.busy": "2025-10-02T21:55:33.237496Z",
     "iopub.status.idle": "2025-10-02T21:56:25.478720Z",
     "shell.execute_reply": "2025-10-02T21:56:25.477742Z",
     "shell.execute_reply.started": "2025-10-02T21:55:33.238256Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 3. Tokenizing Data ---\n",
      "Tokenization complete (Max Length: 128).\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ✅ Step 3: Tokenizer\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 3. Tokenizing Data ---\")\n",
    "\n",
    "# Use DistilBertTokenizerFast for better performance\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)\n",
    "\n",
    "train_encodings = tokenizer(\n",
    "    list(train_texts.astype(str)), \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    max_length=MAX_LENGTH, \n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "val_encodings = tokenizer(\n",
    "    list(val_texts.astype(str)), \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    max_length=MAX_LENGTH, \n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "test_encodings = tokenizer( \n",
    "    list(test_texts.astype(str)), \n",
    "    truncation=True, \n",
    "    padding='max_length', \n",
    "    max_length=MAX_LENGTH, \n",
    "    return_tensors=\"tf\"\n",
    ")\n",
    "\n",
    "print(f\"Tokenization complete (Max Length: {MAX_LENGTH}).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T21:58:36.616534Z",
     "iopub.status.busy": "2025-10-02T21:58:36.615881Z",
     "iopub.status.idle": "2025-10-02T21:58:39.589605Z",
     "shell.execute_reply": "2025-10-02T21:58:39.588779Z",
     "shell.execute_reply.started": "2025-10-02T21:58:36.616509Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 4. Initializing and Compiling Model ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "088cfa743203429780ec2f0a86341a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compilation successful.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ✅ Step 4: Model Initialization and Compile (within Strategy scope)\n",
    "# ==============================================================================\n",
    "with strategy.scope():\n",
    "    print(\"\\n--- 4. Initializing and Compiling Model ---\")\n",
    "\n",
    "    # Load the DistilBERT model configured for sequence classification (2 labels: 0 or 1)\n",
    "    model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)\n",
    "\n",
    "    # Define Optimizer, Loss, and Metrics\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    \n",
    "    # Use SparseCategoricalCrossentropy since labels are integers (0 or 1)\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    \n",
    "    # Use SparseCategoricalAccuracy for the same reason\n",
    "    metrics = [tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")]\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "    print(\"Model compilation successful.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T21:59:04.124546Z",
     "iopub.status.busy": "2025-10-02T21:59:04.124276Z",
     "iopub.status.idle": "2025-10-02T23:28:36.572934Z",
     "shell.execute_reply": "2025-10-02T23:28:36.572130Z",
     "shell.execute_reply.started": "2025-10-02T21:59:04.124527Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 5. Starting Training ---\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1759442369.200674     101 service.cc:148] XLA service 0x78252c58a850 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1759442369.201326     101 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1759442369.286726     101 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1759442369.446748     101 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13071/13071 [==============================] - 1829s 137ms/step - loss: 0.0579 - accuracy: 0.9798 - val_loss: 0.0865 - val_accuracy: 0.9723\n",
      "Epoch 2/3\n",
      "13071/13071 [==============================] - 1774s 136ms/step - loss: 0.0292 - accuracy: 0.9903 - val_loss: 0.0413 - val_accuracy: 0.9856\n",
      "Epoch 3/3\n",
      "13071/13071 [==============================] - 1769s 135ms/step - loss: 0.0277 - accuracy: 0.9913 - val_loss: 0.0572 - val_accuracy: 0.9847\n",
      "Training Complete.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ✅ Step 5: Train the model\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 5. Starting Training ---\")\n",
    "history = model.fit(\n",
    "    x=dict(train_encodings),\n",
    "    y=train_labels.values,\n",
    "    batch_size=BATCH_SIZE * strategy.num_replicas_in_sync, # Adjust batch size for multi-device training\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(dict(val_encodings), val_labels.values)\n",
    ")\n",
    "print(\"Training Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T23:33:54.304527Z",
     "iopub.status.busy": "2025-10-02T23:33:54.304196Z",
     "iopub.status.idle": "2025-10-02T23:36:22.540531Z",
     "shell.execute_reply": "2025-10-02T23:36:22.539723Z",
     "shell.execute_reply.started": "2025-10-02T23:33:54.304505Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 6. Final Evaluation on UNSEEN Test Set ---\n",
      "1634/1634 [==============================] - 79s 48ms/step - loss: 0.0576 - accuracy: 0.9853\n",
      "Test Loss: 0.0576\n",
      "Test Accuracy: 0.9853\n",
      "--------------------------------------------------\n",
      "817/817 [==============================] - 69s 82ms/step\n",
      "\n",
      "--- Detailed Classification Report ---\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Benign (0)       0.98      0.99      0.99     13071\n",
      "Malicious (1)       0.99      0.98      0.99     13071\n",
      "\n",
      "     accuracy                           0.99     26142\n",
      "    macro avg       0.99      0.99      0.99     26142\n",
      " weighted avg       0.99      0.99      0.99     26142\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ✅ Step 6: Evaluate the model (FINAL evaluation on the Test set)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 6. Final Evaluation on UNSEEN Test Set ---\")\n",
    "loss, accuracy = model.evaluate(dict(test_encodings), test_labels.values, batch_size=BATCH_SIZE)\n",
    "print(f\"Test Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Optional: Generate more detailed metrics like Precision, Recall, F1-Score\n",
    "test_predictions = model.predict(dict(test_encodings)).logits\n",
    "test_predictions = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(\"\\n--- Detailed Classification Report ---\")\n",
    "print(classification_report(test_labels, test_predictions, target_names=[\"Benign (0)\", \"Malicious (1)\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-02T23:36:54.412046Z",
     "iopub.status.busy": "2025-10-02T23:36:54.411544Z",
     "iopub.status.idle": "2025-10-02T23:36:55.155168Z",
     "shell.execute_reply": "2025-10-02T23:36:55.154359Z",
     "shell.execute_reply.started": "2025-10-02T23:36:54.412012Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 7. Saving Model and Tokenizer ---\n",
      "Model and Tokenizer successfully saved to './prompt_injection_distilbert_v3'\n",
      "Don't forget to download this folder!\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# ✅ Step 7: Save the model and tokenizer\n",
    "# ==============================================================================\n",
    "print(\"\\n--- 7. Saving Model and Tokenizer ---\")\n",
    "model.save_pretrained(SAVE_PATH)\n",
    "tokenizer.save_pretrained(SAVE_PATH)\n",
    "print(f\"Model and Tokenizer successfully saved to '{SAVE_PATH}'\")\n",
    "print(\"Don't forget to download this folder!\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8391984,
     "sourceId": 13244143,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
